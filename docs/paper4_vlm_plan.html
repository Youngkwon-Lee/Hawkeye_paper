<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Paper 4: VLM for PD Motor Assessment — Detailed Plan</title>
<style>
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    font-family: 'Segoe UI', -apple-system, sans-serif;
    background: #fff; color: #1a1a1a;
    line-height: 1.7; padding: 48px 64px;
    max-width: 1200px; margin: 0 auto;
  }
  h1 { font-size: 28px; font-weight: 700; color: #111; margin-bottom: 4px; }
  .subtitle { font-size: 15px; color: #666; margin-bottom: 8px; }
  .meta { font-size: 13px; color: #888; margin-bottom: 36px; padding-bottom: 16px; border-bottom: 2px solid #111; }
  h2 {
    font-size: 20px; font-weight: 700; margin: 44px 0 16px 0; color: #111;
    padding-bottom: 6px; border-bottom: 2px solid #e0e0e0;
  }
  h3 { font-size: 16px; font-weight: 600; margin: 28px 0 10px 0; color: #222; }
  h4 { font-size: 14px; font-weight: 600; margin: 18px 0 8px 0; color: #333; }
  p { font-size: 14px; color: #333; margin: 8px 0; }
  table { width: 100%; border-collapse: collapse; margin: 12px 0 20px 0; font-size: 13px; }
  thead th {
    background: #f5f5f5; border: 1px solid #ddd; padding: 10px 12px;
    text-align: left; font-weight: 600; color: #333;
  }
  tbody td { border: 1px solid #ddd; padding: 9px 12px; vertical-align: top; }
  tbody tr:hover { background: #fafafa; }
  .section { margin-bottom: 44px; page-break-inside: avoid; }
  .tag { display: inline-block; padding: 2px 10px; border-radius: 12px; font-size: 11px; font-weight: 600; margin: 1px 2px; }
  .tag-green { background: #e8f5e9; color: #2e7d32; }
  .tag-red { background: #ffebee; color: #c62828; }
  .tag-blue { background: #e3f2fd; color: #1565c0; }
  .tag-orange { background: #fff3e0; color: #e65100; }
  .tag-purple { background: #f3e5f5; color: #7b1fa2; }
  .tag-gray { background: #f5f5f5; color: #616161; }
  .box {
    padding: 16px 20px; margin: 14px 0; border-radius: 0 6px 6px 0;
  }
  .box-blue { background: #f0f7ff; border-left: 4px solid #1976d2; }
  .box-green { background: #f0faf0; border-left: 4px solid #43a047; }
  .box-yellow { background: #fffde7; border-left: 4px solid #f9a825; }
  .box-red { background: #fff5f5; border-left: 4px solid #e53935; }
  .box-purple { background: #faf0ff; border-left: 4px solid #8e24aa; }
  .card-grid { display: grid; gap: 14px; margin: 14px 0; }
  .card-grid-2 { grid-template-columns: 1fr 1fr; }
  .card-grid-3 { grid-template-columns: 1fr 1fr 1fr; }
  .card {
    border: 1px solid #e0e0e0; border-radius: 8px; padding: 18px; background: #fafafa;
  }
  .card h4 { margin-top: 0; }
  ul, ol { margin: 6px 0 6px 24px; font-size: 14px; }
  li { margin: 3px 0; }
  .badge {
    display: inline-block; background: #1976d2; color: white;
    padding: 3px 12px; border-radius: 16px; font-size: 12px; font-weight: 600;
  }
  .badge-green { background: #43a047; }
  .badge-orange { background: #e65100; }
  .badge-red { background: #c62828; }
  .flow {
    display: flex; align-items: center; gap: 0; margin: 16px 0; flex-wrap: wrap;
  }
  .flow-item {
    padding: 10px 18px; border-radius: 6px; font-size: 13px; font-weight: 500;
    border: 1px solid #ddd; background: #f8f9fa; text-align: center;
  }
  .flow-arrow {
    font-size: 18px; color: #999; padding: 0 6px; font-weight: 700;
  }
  .flow-active { background: #e3f2fd; border-color: #1976d2; color: #1565c0; }
  .flow-best { background: #e8f5e9; border-color: #43a047; color: #2e7d32; }
  .strategy-diagram {
    display: grid; grid-template-columns: 120px 1fr; gap: 2px; margin: 12px 0;
    font-size: 12px;
  }
  .strategy-label {
    background: #f5f5f5; padding: 10px; font-weight: 600; display: flex;
    align-items: center; border-radius: 4px 0 0 4px;
  }
  .strategy-content {
    background: #fafafa; padding: 10px; border-radius: 0 4px 4px 0;
    border: 1px solid #eee; font-family: 'Consolas', monospace; font-size: 12px;
    line-height: 1.5;
  }
  .matrix-cell-yes { background: #e8f5e9; text-align: center; font-weight: 600; color: #2e7d32; }
  .matrix-cell-no { background: #f5f5f5; text-align: center; color: #999; }
  .matrix-cell-partial { background: #fff8e1; text-align: center; color: #e65100; }
  .timeline {
    position: relative; margin: 20px 0 20px 20px; padding-left: 24px;
    border-left: 3px solid #e0e0e0;
  }
  .timeline-item {
    position: relative; margin-bottom: 20px; padding: 14px 18px;
    background: #fafafa; border-radius: 6px; border: 1px solid #eee;
  }
  .timeline-item::before {
    content: ''; position: absolute; left: -33px; top: 18px;
    width: 12px; height: 12px; border-radius: 50%;
    background: #1976d2; border: 2px solid #fff;
  }
  .timeline-date { font-weight: 600; color: #1976d2; font-size: 13px; }
  .timeline-desc { font-size: 13px; margin-top: 4px; }
  .num-highlight {
    display: inline-block; font-size: 32px; font-weight: 700;
    margin-right: 6px; line-height: 1;
  }
  .summary-grid { display: grid; grid-template-columns: repeat(4, 1fr); gap: 12px; margin: 20px 0; }
  .summary-card {
    text-align: center; padding: 18px 10px; border-radius: 8px;
    border: 1px solid #e0e0e0;
  }
  .summary-card .number { font-size: 26px; font-weight: 700; }
  .summary-card .label { font-size: 11px; color: #666; margin-top: 4px; }
  code {
    background: #f0f0f0; padding: 2px 6px; border-radius: 3px;
    font-family: 'Consolas', monospace; font-size: 12px;
  }
  .prompt-box {
    background: #1a1a2e; color: #e0e0e0; padding: 16px 20px;
    border-radius: 6px; margin: 12px 0; font-family: 'Consolas', monospace;
    font-size: 12.5px; line-height: 1.6; white-space: pre-wrap;
  }
  .prompt-box .role { color: #64b5f6; font-weight: 600; }
  .prompt-box .highlight { color: #81c784; }
  @media print {
    body { padding: 24px; font-size: 12px; }
    .section { page-break-inside: avoid; }
  }
</style>
</head>
<body>

<h1>Paper 4: VLM for Parkinson's Motor Assessment</h1>
<p class="subtitle">Can Vision-Language Models Score MDS-UPDRS? A Systematic Study of Input Strategies, Model Types, and Adaptation Methods</p>
<p class="meta">Hawkeye Research Team | Target: Nature Medicine / Lancet Digital Health / MICCAI 2026 | Created: 2026-02-09</p>

<!-- Summary Cards -->
<div class="summary-grid">
  <div class="summary-card" style="border-color:#c62828;">
    <div class="number" style="color:#c62828;">0</div>
    <div class="label">Existing VLM+PD Papers</div>
  </div>
  <div class="summary-card" style="border-color:#1976d2;">
    <div class="number" style="color:#1976d2;">6</div>
    <div class="label">Input Strategies</div>
  </div>
  <div class="summary-card" style="border-color:#7b1fa2;">
    <div class="number" style="color:#7b1fa2;">6</div>
    <div class="label">VLM Models</div>
  </div>
  <div class="summary-card" style="border-color:#43a047;">
    <div class="number" style="color:#43a047;">3</div>
    <div class="label">Adaptation Levels</div>
  </div>
</div>

<!-- Section 1: Why VLM -->
<div class="section">
<h2>1. Why VLM for PD Motor Assessment?</h2>

<div class="box box-green">
  <strong>Core Question:</strong> MDS-UPDRS Part III 운동 평가를 VLM이 수행할 수 있는가?<br>
  <strong>Existing Papers:</strong> 0편 (세계 최초 연구 기회)<br>
  <strong>Unique Value:</strong> 점수 예측 + 임상적 근거 설명 (Explainable AI)
</div>

<h3>1.1 VLM vs Traditional Approaches</h3>
<table>
  <thead>
    <tr><th>차원</th><th>Traditional ML</th><th>AQA Methods</th><th>VLM (Ours)</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>학습 데이터 필요</strong></td>
      <td>필수 (100+ 영상)</td>
      <td>필수 (100+ 영상)</td>
      <td>Zero-shot 가능</td>
    </tr>
    <tr>
      <td><strong>새 Task 적응</strong></td>
      <td>재학습 필요</td>
      <td>재학습 필요</td>
      <td>프롬프트 변경만</td>
    </tr>
    <tr>
      <td><strong>설명 가능성</strong></td>
      <td>Feature importance (제한적)</td>
      <td>Grad-CAM (시각적)</td>
      <td><strong>자연어 설명 (임상 수준)</strong></td>
    </tr>
    <tr>
      <td><strong>정확도 (예상)</strong></td>
      <td>r=0.55~0.81 (검증됨)</td>
      <td>미검증</td>
      <td>r=0.3~0.6 (예상)</td>
    </tr>
    <tr>
      <td><strong>임상 수용성</strong></td>
      <td>낮음 (black-box)</td>
      <td>낮음</td>
      <td><strong>높음 (이유 설명)</strong></td>
    </tr>
  </tbody>
</table>

<h3>1.2 Paper Title Options</h3>
<div class="card-grid card-grid-2">
  <div class="card" style="border-left: 4px solid #1976d2;">
    <h4>Option A (Medical Focus)</h4>
    <p><em>"Can Vision-Language Models Score Parkinson's Disease? Zero-Shot MDS-UPDRS Motor Assessment with Multimodal Prompting"</em></p>
    <p>Target: Nature Medicine, Lancet Digital Health</p>
  </div>
  <div class="card" style="border-left: 4px solid #7b1fa2;">
    <h4>Option B (CV + Medical)</h4>
    <p><em>"What Should VLMs See? A Systematic Study of Input Strategies for Clinical Motor Assessment in Parkinson's Disease"</em></p>
    <p>Target: MICCAI 2026, CVPR Workshop</p>
  </div>
</div>
</div>

<!-- Section 2: Models -->
<div class="section">
<h2>2. Model Selection: Commercial + Open-Source</h2>

<h3>2.1 Commercial VLMs (Upper Bound)</h3>
<table>
  <thead>
    <tr><th>Model</th><th>Video Input</th><th>Context</th><th>Few-shot?</th><th>Fine-tune?</th><th>비용/query</th><th>용도</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>GPT-4o / GPT-5</strong></td>
      <td><span class="tag tag-red">프레임만</span></td>
      <td>128K</td>
      <td><span class="tag tag-orange">제한적</span><br>(토큰 한계)</td>
      <td><span class="tag tag-red">불가</span></td>
      <td>~$1.30</td>
      <td>Zero-shot only</td>
    </tr>
    <tr>
      <td><strong>Gemini 2.5 Pro</strong></td>
      <td><span class="tag tag-green">네이티브</span></td>
      <td>2M</td>
      <td><span class="tag tag-green">가능</span><br>(10 videos)</td>
      <td><span class="tag tag-red">불가</span></td>
      <td>~$0.15</td>
      <td>Zero + Few-shot</td>
    </tr>
    <tr>
      <td><strong>Gemini 2.5 Flash-Lite</strong></td>
      <td><span class="tag tag-green">네이티브</span></td>
      <td>1M</td>
      <td><span class="tag tag-green">가능</span></td>
      <td><span class="tag tag-red">불가</span></td>
      <td>~$0.004</td>
      <td>대량 실험용</td>
    </tr>
  </tbody>
</table>

<h3>2.2 Open-Source VLMs (Practical Bound + Fine-tunable)</h3>
<table>
  <thead>
    <tr><th>Model</th><th>Video Input</th><th>Size</th><th>VRAM</th><th>Few-shot?</th><th>Fine-tune?</th><th>특징</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Qwen2.5-VL-7B</strong></td>
      <td><span class="tag tag-green">네이티브</span></td>
      <td>7B</td>
      <td>24GB</td>
      <td><span class="tag tag-green">가능</span></td>
      <td><span class="tag tag-green">LoRA</span></td>
      <td>최신, Dynamic FPS, 비디오 특화</td>
    </tr>
    <tr>
      <td><strong>InternVL2.5-8B</strong></td>
      <td><span class="tag tag-green">네이티브</span></td>
      <td>8B</td>
      <td>24GB</td>
      <td><span class="tag tag-green">가능</span></td>
      <td><span class="tag tag-green">LoRA</span></td>
      <td>강력한 비전 인코더, 다국어</td>
    </tr>
    <tr>
      <td><strong>LLaVA-Video-7B</strong></td>
      <td><span class="tag tag-green">네이티브</span></td>
      <td>7B</td>
      <td>20GB</td>
      <td><span class="tag tag-green">가능</span></td>
      <td><span class="tag tag-green">LoRA</span></td>
      <td>Video 특화 아키텍처</td>
    </tr>
  </tbody>
</table>

<h3>2.3 왜 둘 다 필요한가</h3>
<div class="card-grid card-grid-2">
  <div class="card" style="border-left: 4px solid #e65100;">
    <h4>Commercial = 성능 상한선 (Upper Bound)</h4>
    <ul>
      <li>"최고 VLM으로도 이 정도 성능"</li>
      <li>재현성 문제 (모델 버전 변경)</li>
      <li>의료 데이터 외부 전송 우려</li>
      <li>Fine-tuning 불가 → zero/few-shot 한계</li>
    </ul>
  </div>
  <div class="card" style="border-left: 4px solid #43a047;">
    <h4>Open-Source = 실용적 한계선 (Practical Bound)</h4>
    <ul>
      <li>"병원에서 실제 배포 가능한 수준"</li>
      <li>완전한 재현성 (모델 고정)</li>
      <li>로컬 실행 → 환자 데이터 보호</li>
      <li>LoRA fine-tuning → 성능 개선 가능</li>
    </ul>
  </div>
</div>

<div class="box box-yellow">
  <strong>핵심 Finding (예상):</strong> Commercial-OpenSource gap 크기가 중요한 결과.<br>
  &bull; Gap 작으면 → "오픈소스로 충분, 병원 즉시 배포 가능"<br>
  &bull; Gap 크면 → "LoRA fine-tuning으로 gap 줄일 수 있는지" 검증
</div>
</div>

<!-- Section 3: Input Strategies -->
<div class="section">
<h2>3. Input Strategies — "VLM에게 무엇을 보여줄 것인가?"</h2>

<div class="box box-purple">
  <strong>이 논문의 핵심 Contribution.</strong> 단순히 VLM으로 점수 예측이 아니라,<br>
  <strong>"어떤 정보를 어떤 형태로 제공해야 VLM이 임상 판단을 가장 잘 하는가?"</strong>의 체계적 비교.
</div>

<h3>3.1 Six Input Strategies</h3>

<!-- Strategy A -->
<div style="margin: 18px 0;">
<h4>Strategy A: Raw Frames Only</h4>
<div class="strategy-diagram">
  <div class="strategy-label" style="background:#e3f2fd;">A. Raw Frames</div>
  <div class="strategy-content">
[Frame_1.jpg] [Frame_4.jpg] [Frame_8.jpg] ... [Frame_16.jpg]
+ Prompt: "Score this finger tapping 0-4 based on MDS-UPDRS criteria"
  </div>
</div>
<p><span class="tag tag-blue">Baseline</span> VLM의 순수 시각 이해 능력 테스트. 추가 정보 없이 RGB만으로 판단.</p>
</div>

<!-- Strategy B -->
<div style="margin: 18px 0;">
<h4>Strategy B: Skeleton Overlay (RGB + Skeleton)</h4>
<div class="strategy-diagram">
  <div class="strategy-label" style="background:#e8f5e9;">B. Skeleton<br>Overlay</div>
  <div class="strategy-content">
[Frame+Skeleton_1.jpg] [Frame+Skeleton_4.jpg] ... [Frame+Skeleton_16.jpg]
  (MediaPipe/MotionBERT 관절 포인트가 영상 위에 그려진 이미지)
+ Prompt: "Observe the joint movements highlighted in the overlay..."
  </div>
</div>
<p><span class="tag tag-green">Attention Guide</span> Skeleton을 시각적으로 겹쳐 VLM의 관절 주목을 유도. RGB 정보는 유지.</p>
</div>

<!-- Strategy C -->
<div style="margin: 18px 0;">
<h4>Strategy C: Skeleton Only (No RGB)</h4>
<div class="strategy-diagram">
  <div class="strategy-label" style="background:#f3e5f5;">C. Skeleton<br>Only</div>
  <div class="strategy-content">
[StickFigure_1.jpg] [StickFigure_4.jpg] ... [StickFigure_16.jpg]
  (검은 배경에 관절 연결선만 표시, 환자 모습 없음)
+ Prompt: "Score the movement quality from this skeleton visualization..."
  </div>
</div>
<p><span class="tag tag-purple">Privacy-Preserving</span> 환자 얼굴/몸 노출 없이 분석. 의료 데이터 공유에 유리. 정보 손실 vs 프라이버시 trade-off 검증.</p>
</div>

<!-- Strategy D -->
<div style="margin: 18px 0;">
<h4>Strategy D: Frames + Kinematic Features (Text)</h4>
<div class="strategy-diagram">
  <div class="strategy-label" style="background:#fff3e0;">D. Frames +<br>Kinematics</div>
  <div class="strategy-content">
[Frame_1.jpg] ... [Frame_16.jpg]
+ Text: "Extracted kinematic features:
   peak_velocity_mean: 2.31 (norm: 3.5-5.0)
   amplitude_mean: 0.045 (norm: 0.08-0.12)
   rhythm_variability: 0.18 (norm: 0.05-0.10)
   fatigue_rate: -0.032 (norm: 0.0-0.01)
   decrement_ratio: 0.72 (norm: 0.90-1.00)"
  </div>
</div>
<p><span class="tag tag-orange">Multi-modal Fusion</span> 시각 정보 + 정량적 수치. VLM이 숫자 데이터를 어떻게 활용하는지 검증.</p>
</div>

<!-- Strategy E -->
<div style="margin: 18px 0;">
<h4>Strategy E: Clinical Chain-of-Thought (CoT)</h4>
<div class="strategy-diagram">
  <div class="strategy-label" style="background:#e8f5e9; color:#2e7d32;">E. Clinical<br>CoT</div>
  <div class="strategy-content">
[Frame_1.jpg] ... [Frame_16.jpg]
+ Prompt: "Evaluate step by step following MDS-UPDRS protocol:
   Step 1: Observe movement SPEED (normal / mildly slow / moderately slow / severely slow)
   Step 2: Assess AMPLITUDE (full range / mildly reduced / moderately reduced / severely reduced)
   Step 3: Check for DECREMENT (amplitude decreases over repetitions?)
   Step 4: Note any HESITATIONS or ARRESTS (freezing moments?)
   Step 5: Check RHYTHM regularity (steady / irregular / very irregular)
   Final: Based on Steps 1-5, assign UPDRS score 0-4 with reasoning."
  </div>
</div>
<p><span class="tag tag-green">Best Expected</span> MDS-UPDRS 임상 평가 프로토콜을 그대로 프롬프트화. VLM의 구조화된 추론 유도. Hallucination 감소 기대.</p>
</div>

<!-- Strategy F -->
<div style="margin: 18px 0;">
<h4>Strategy F: Multi-turn Dialogue</h4>
<div class="strategy-diagram">
  <div class="strategy-label" style="background:#fce4ec;">F. Multi-turn<br>Dialogue</div>
  <div class="strategy-content">
Turn 1: [Frames] + "Describe in detail what you observe in this movement."
VLM → "The patient's fingers open and close with moderate speed.
        The amplitude appears to decrease after the 5th repetition..."

Turn 2: "Based on your observation above, assign MDS-UPDRS score 0-4.
         Explain which specific observations support your score."
VLM → "Score: 2. Moderate bradykinesia observed because..."
  </div>
</div>
<p><span class="tag tag-red">Human Mimic</span> 관찰→판단을 분리. 인간 신경과 전문의의 진단 과정 모방. 관찰 단계의 정확성을 별도 평가 가능.</p>
</div>

<h3>3.2 Strategy 비교 요약</h3>
<table>
  <thead>
    <tr>
      <th>Strategy</th>
      <th>입력 구성</th>
      <th>테스트 가설</th>
      <th>장점</th>
      <th>단점</th>
      <th>예상 성능</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>A: Raw Frames</strong></td>
      <td>RGB 이미지 8-16장</td>
      <td>순수 시각만으로 가능한가?</td>
      <td>가장 단순, Baseline</td>
      <td>VLM이 뭘 봐야 할지 모름</td>
      <td>r~0.25-0.35</td>
    </tr>
    <tr>
      <td><strong>B: Skeleton Overlay</strong></td>
      <td>RGB + 관절 시각화</td>
      <td>Skeleton이 attention 유도?</td>
      <td>관절 위치 명시적</td>
      <td>Overlay가 RGB 정보 가림</td>
      <td>r~0.30-0.40</td>
    </tr>
    <tr>
      <td><strong>C: Skeleton Only</strong></td>
      <td>Stick figure만</td>
      <td>RGB 없이 가능한가?</td>
      <td>프라이버시 보존</td>
      <td>떨림, 표정 정보 손실</td>
      <td>r~0.20-0.30</td>
    </tr>
    <tr>
      <td><strong>D: + Kinematic Text</strong></td>
      <td>RGB + 정량 수치</td>
      <td>수치 데이터가 도움?</td>
      <td>정확한 측정값 제공</td>
      <td>VLM이 수치 해석 잘 하는지?</td>
      <td>r~0.35-0.50</td>
    </tr>
    <tr style="background:#f0faf0;">
      <td><strong>E: Clinical CoT</strong></td>
      <td>RGB + 단계별 평가</td>
      <td>구조화된 추론 효과?</td>
      <td>Hallucination 감소, 임상 프로토콜</td>
      <td>프롬프트 길어짐</td>
      <td><strong>r~0.35-0.50</strong></td>
    </tr>
    <tr>
      <td><strong>F: Multi-turn</strong></td>
      <td>RGB + 2단계 대화</td>
      <td>관찰-판단 분리 효과?</td>
      <td>인간 진단 과정 모방</td>
      <td>API 호출 2배, 비용 증가</td>
      <td>r~0.30-0.45</td>
    </tr>
  </tbody>
</table>
</div>

<!-- Section 4: Adaptation Levels -->
<div class="section">
<h2>4. Adaptation Levels — Zero-shot → Few-shot → LoRA</h2>

<h3>4.1 Three-Level Progression</h3>
<div class="flow">
  <div class="flow-item">Level 0<br><strong>Zero-shot</strong><br><span style="font-size:11px;">학습 없음</span></div>
  <div class="flow-arrow">&rarr;</div>
  <div class="flow-item flow-active">Level 1<br><strong>Few-shot ICL</strong><br><span style="font-size:11px;">예시 제공</span></div>
  <div class="flow-arrow">&rarr;</div>
  <div class="flow-item flow-best">Level 2<br><strong>LoRA Fine-tune</strong><br><span style="font-size:11px;">가중치 업데이트</span></div>
  <div class="flow-arrow">&rarr;</div>
  <div class="flow-item" style="background:#fff3e0; border-color:#e65100;">Comparison<br><strong>Traditional ML</strong><br><span style="font-size:11px;">CORAL, Mamba</span></div>
</div>

<h3>4.2 각 Level 상세</h3>

<div class="card-grid card-grid-3">
  <div class="card" style="border-top: 3px solid #1976d2;">
    <h4>Level 0: Zero-shot</h4>
    <p><strong>적용:</strong> 모든 모델 (Commercial + Open-source)</p>
    <p><strong>방법:</strong> MDS-UPDRS 채점 기준만 프롬프트로 제공</p>
    <p><strong>의미:</strong> VLM의 사전학습 지식만으로 얼마나 가능한지</p>
    <p><strong>핵심 질문:</strong> "VLM이 PD 운동 장애를 '알고' 있는가?"</p>
    <ul>
      <li>추가 데이터 불필요</li>
      <li>가장 빠른 실험</li>
      <li>Baseline 성능</li>
    </ul>
  </div>
  <div class="card" style="border-top: 3px solid #43a047;">
    <h4>Level 1: Few-shot ICL</h4>
    <p><strong>적용:</strong> Gemini + Open-source만<br>(GPT-4o는 비디오 토큰 제한)</p>
    <p><strong>방법:</strong> 1-shot, 3-shot, 5-shot</p>
    <p><strong>의미:</strong> 예시가 있으면 얼마나 개선되는지</p>
    <p><strong>핵심 질문:</strong> "몇 개의 예시가 필요한가?"</p>
    <ul>
      <li>가중치 변경 없음</li>
      <li>Gemini: 네이티브 비디오로 가능</li>
      <li>Scaling curve 분석</li>
    </ul>
  </div>
  <div class="card" style="border-top: 3px solid #e65100;">
    <h4>Level 2: LoRA Fine-tune</h4>
    <p><strong>적용:</strong> Open-source만<br>(Qwen2.5-VL-7B 우선)</p>
    <p><strong>방법:</strong> PD4T train set으로 LoRA (rank 8-16)</p>
    <p><strong>의미:</strong> 도메인 적응 후 성능 상한</p>
    <p><strong>핵심 질문:</strong> "Fine-tuning이 Traditional ML 수준 도달 가능?"</p>
    <ul>
      <li>HPC A100 1장 (24GB)</li>
      <li>100-200 영상으로 학습</li>
      <li>Explainability 유지 여부</li>
    </ul>
  </div>
</div>

<h3>4.3 모델별 적용 가능 Level</h3>
<table>
  <thead>
    <tr><th>Model</th><th>Zero-shot</th><th>Few-shot (3)</th><th>LoRA Fine-tune</th><th>제한 사유</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>GPT-4o / GPT-5</strong></td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-partial">△</td>
      <td class="matrix-cell-no">X</td>
      <td>비디오 네이티브 X, 토큰 비용 높음, Fine-tune 불가</td>
    </tr>
    <tr>
      <td><strong>Gemini 2.5 Pro</strong></td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-no">X</td>
      <td>Fine-tune 불가 (API만)</td>
    </tr>
    <tr>
      <td><strong>Gemini 2.5 Flash-Lite</strong></td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-no">X</td>
      <td>Fine-tune 불가, 저비용 대량 실험용</td>
    </tr>
    <tr>
      <td><strong>Qwen2.5-VL-7B</strong></td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-yes">O</td>
      <td>제한 없음 (Full 지원)</td>
    </tr>
    <tr>
      <td><strong>InternVL2.5-8B</strong></td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-yes">O</td>
      <td>제한 없음</td>
    </tr>
    <tr>
      <td><strong>LLaVA-Video-7B</strong></td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-yes">O</td>
      <td class="matrix-cell-yes">O</td>
      <td>제한 없음</td>
    </tr>
  </tbody>
</table>

<h3>4.4 LoRA Fine-tuning 세부 계획</h3>
<div class="box box-blue">
  <strong>Target Model:</strong> Qwen2.5-VL-7B<br>
  <strong>Method:</strong> LoRA (rank=16, alpha=32, dropout=0.05)<br>
  <strong>Data:</strong> PD4T train split (~150 videos, 4 tasks)<br>
  <strong>Evaluation:</strong> PD4T test split + TULIP (external validation)<br>
  <strong>Hardware:</strong> HPC A100 40GB x 1<br>
  <strong>Training:</strong> ~3-5 epochs, batch_size=4, lr=2e-5<br>
  <strong>Key Question:</strong> Fine-tuning 후에도 reasoning 설명 능력 유지되는가?
</div>
</div>

<!-- Section 5: Experiment Matrix -->
<div class="section">
<h2>5. Full Experiment Matrix</h2>

<h3>5.1 Main Experiments (Strategy x Level x Model)</h3>
<table>
  <thead>
    <tr>
      <th>Exp</th>
      <th>Strategy</th>
      <th>Level</th>
      <th>Models</th>
      <th>조건수</th>
      <th>Phase</th>
    </tr>
  </thead>
  <tbody>
    <tr style="background:#f0f7ff;">
      <td><strong>E1</strong></td>
      <td>A (Raw Frames)</td>
      <td>Zero-shot</td>
      <td>GPT-4o, Gemini Pro, Flash-Lite, Qwen, InternVL, LLaVA</td>
      <td>6</td>
      <td>Phase 1</td>
    </tr>
    <tr style="background:#f0f7ff;">
      <td><strong>E2</strong></td>
      <td>E (Clinical CoT)</td>
      <td>Zero-shot</td>
      <td>GPT-4o, Gemini Pro, Flash-Lite, Qwen, InternVL, LLaVA</td>
      <td>6</td>
      <td>Phase 1</td>
    </tr>
    <tr style="background:#f0f7ff;">
      <td><strong>E3</strong></td>
      <td>B (Skeleton Overlay)</td>
      <td>Zero-shot</td>
      <td>GPT-4o, Gemini Pro, Qwen</td>
      <td>3</td>
      <td>Phase 1</td>
    </tr>
    <tr>
      <td><strong>E4</strong></td>
      <td>C (Skeleton Only)</td>
      <td>Zero-shot</td>
      <td>Gemini Pro, Qwen</td>
      <td>2</td>
      <td>Phase 2</td>
    </tr>
    <tr>
      <td><strong>E5</strong></td>
      <td>D (+ Kinematic Text)</td>
      <td>Zero-shot</td>
      <td>GPT-4o, Gemini Pro, Qwen</td>
      <td>3</td>
      <td>Phase 2</td>
    </tr>
    <tr>
      <td><strong>E6</strong></td>
      <td>F (Multi-turn)</td>
      <td>Zero-shot</td>
      <td>GPT-4o, Gemini Pro, Qwen</td>
      <td>3</td>
      <td>Phase 2</td>
    </tr>
    <tr style="background:#f0faf0;">
      <td><strong>E7</strong></td>
      <td>A, E (Best 2)</td>
      <td>Few-shot (1,3,5)</td>
      <td>Gemini Pro, Qwen</td>
      <td>12</td>
      <td>Phase 2</td>
    </tr>
    <tr style="background:#fff8e1;">
      <td><strong>E8</strong></td>
      <td>A, E (Best 2)</td>
      <td>LoRA Fine-tune</td>
      <td>Qwen2.5-VL-7B</td>
      <td>2</td>
      <td>Phase 3</td>
    </tr>
    <tr style="background:#fce4ec;">
      <td><strong>E9</strong></td>
      <td>Frame ablation</td>
      <td>Zero-shot</td>
      <td>Best model from E1-E2</td>
      <td>4<br>(4,8,16,32 frames)</td>
      <td>Phase 2</td>
    </tr>
  </tbody>
</table>

<p><strong>총 실험 조건:</strong> ~41개 조건 x 4 tasks = ~164 실험 (일부 겹침 제외 실제 ~120개)</p>

<h3>5.2 Baseline Comparisons (E10)</h3>
<table>
  <thead>
    <tr><th>Baseline</th><th>Type</th><th>설명</th></tr>
  </thead>
  <tbody>
    <tr><td>Random</td><td>통계</td><td>0-4 무작위 배정</td></tr>
    <tr><td>Majority Class</td><td>통계</td><td>가장 빈도 높은 점수만 예측</td></tr>
    <tr><td>AQA (CoRe/TSA)</td><td>Supervised</td><td>Action Quality Assessment 방법론, PD4T로 학습</td></tr>
    <tr><td>CORAL Ordinal (Ours)</td><td>Supervised</td><td>현재 production 모델 (Pearson 0.55-0.81)</td></tr>
    <tr><td>Mamba Enhanced (Ours)</td><td>Supervised</td><td>최고 실험 성능 (Pearson 0.61-0.80)</td></tr>
  </tbody>
</table>

<h3>5.3 Clinical Reasoning Evaluation (E11)</h3>
<div class="box box-green">
  <strong>목적:</strong> VLM이 생성한 scoring reasoning의 임상적 타당성 평가<br><br>
  <strong>방법:</strong><br>
  &bull; Best VLM (E1-E8에서 선정)의 reasoning output 수집 (100건)<br>
  &bull; Neurologist 3명이 blind review<br>
  &bull; 평가 기준 (Likert 1-5):<br>
  &nbsp;&nbsp;(1) 관찰 정확성: VLM이 영상을 정확히 관찰했는가?<br>
  &nbsp;&nbsp;(2) 임상 논리: UPDRS 기준에 맞는 논리인가?<br>
  &nbsp;&nbsp;(3) 유용성: 실제 임상에서 참고할 수 있는 수준인가?<br><br>
  <strong>이것이 논문의 가장 강력한 contribution이 될 수 있음</strong>
</div>
</div>

<!-- Section 6: Prompt Examples -->
<div class="section">
<h2>6. Prompt Design Examples</h2>

<h3>6.1 Strategy A: Zero-shot Raw Frames</h3>
<div class="prompt-box"><span class="role">[System]</span>
You are an expert neurologist certified in MDS-UPDRS Part III motor examination.
You will assess a patient's motor function from video frames.

<span class="role">[User]</span>
The following images show a patient performing <span class="highlight">finger tapping</span> (MDS-UPDRS Item 3.4).
The patient repeatedly taps index finger and thumb as quickly and as big as possible.

Score from 0-4:
  0: Normal speed, amplitude, no hesitations
  1: Any ONE: slight slowing, mild amplitude reduction, or decrement
  2: Any ONE: moderate slowing, moderate amplitude reduction
  3: Severe slowing with frequent hesitations or arrests
  4: Can barely perform the task

[image_1.jpg] [image_2.jpg] ... [image_16.jpg]

Provide: (1) Your score (0-4), (2) Brief clinical reasoning.</div>

<h3>6.2 Strategy E: Clinical CoT</h3>
<div class="prompt-box"><span class="role">[System]</span>
You are an MDS-UPDRS certified rater. Evaluate the patient step by step.

<span class="role">[User]</span>
Observe this <span class="highlight">finger tapping</span> video and evaluate each dimension:

[image_1.jpg] ... [image_16.jpg]

<span class="highlight">Step 1 - SPEED:</span> Is the tapping speed normal, mildly slow, moderately slow, or severely slow?
<span class="highlight">Step 2 - AMPLITUDE:</span> Is the finger opening full range, mildly reduced, moderately reduced, or minimal?
<span class="highlight">Step 3 - DECREMENT:</span> Does the amplitude decrease over 10 repetitions? If so, how much?
<span class="highlight">Step 4 - HESITATIONS:</span> Are there any freezing moments or arrests during tapping?
<span class="highlight">Step 5 - RHYTHM:</span> Is the rhythm regular, slightly irregular, or very irregular?

Based on Steps 1-5, assign MDS-UPDRS score (0-4) with final reasoning.</div>

<h3>6.3 Strategy D: Frames + Kinematic Text</h3>
<div class="prompt-box"><span class="role">[System]</span>
You are an expert neurologist. Use both the video frames AND the extracted
kinematic measurements to make your assessment.

<span class="role">[User]</span>
Patient performing <span class="highlight">finger tapping</span>:

[image_1.jpg] ... [image_16.jpg]

<span class="highlight">Extracted kinematic features (from pose estimation):</span>
  - Peak velocity: 2.31 units/frame  (healthy norm: 3.5-5.0)
  - Mean amplitude: 0.045 units      (healthy norm: 0.08-0.12)
  - Rhythm CV: 0.18                  (healthy norm: 0.05-0.10)
  - Fatigue rate: -0.032/rep         (healthy norm: > -0.01)
  - Decrement ratio: 0.72            (healthy norm: > 0.90)

Score 0-4 with reasoning, integrating both visual and kinematic evidence.</div>
</div>

<!-- Section 7: Dataset & Feasibility -->
<div class="section">
<h2>7. Dataset & Practical Feasibility</h2>

<h3>7.1 사용 가능한 데이터</h3>
<table>
  <thead>
    <tr><th>Dataset</th><th>Raw Video?</th><th>Tasks</th><th>Subjects</th><th>VLM 활용</th><th>역할</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>PD4T</strong></td>
      <td><span class="tag tag-green">O (mp4)</span></td>
      <td>Gait, FT, Hand, Leg</td>
      <td>50 PD</td>
      <td><span class="tag tag-green">가능</span></td>
      <td>Main (train/test)</td>
    </tr>
    <tr>
      <td><strong>TULIP</strong></td>
      <td><span class="tag tag-green">O (mp4)</span></td>
      <td>Gait, FT, Hand, Leg</td>
      <td>15 PD</td>
      <td><span class="tag tag-green">가능</span></td>
      <td>External validation</td>
    </tr>
    <tr>
      <td>UDysRS</td>
      <td><span class="tag tag-red">X (skeleton only)</span></td>
      <td>Comm, LA, TT</td>
      <td>79 PD</td>
      <td><span class="tag tag-red">불가</span></td>
      <td>-</td>
    </tr>
    <tr>
      <td>CARE-PD</td>
      <td><span class="tag tag-orange">제한적</span></td>
      <td>Gait only</td>
      <td>100+</td>
      <td><span class="tag tag-orange">확인 필요</span></td>
      <td>추가 검증용 (가능 시)</td>
    </tr>
  </tbody>
</table>

<div class="box box-yellow">
  <strong>실험 규모:</strong> PD4T ~200 videos (50 subjects x 4 tasks) + TULIP ~60 videos (15 subjects x 4 tasks)<br>
  <strong>총:</strong> ~260 videos for evaluation. Zero-shot과 few-shot에 충분한 규모.
</div>

<h3>7.2 Frame Sampling Strategy</h3>
<table>
  <thead>
    <tr><th>방법</th><th>설명</th><th>프레임 수</th><th>장점</th><th>단점</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Uniform</strong></td>
      <td>등간격 샘플링</td>
      <td>8, 16, 32</td>
      <td>구현 단순, 재현 가능</td>
      <td>중요 순간 놓칠 수 있음</td>
    </tr>
    <tr>
      <td><strong>Motion-based</strong></td>
      <td>움직임 변화량 기반 선택</td>
      <td>8-16</td>
      <td>핵심 동작 포착</td>
      <td>구현 복잡</td>
    </tr>
    <tr>
      <td><strong>Keyframe</strong></td>
      <td>시작/중간/끝 + 피크 동작</td>
      <td>8-12</td>
      <td>temporal 구조 보존</td>
      <td>task별 설계 필요</td>
    </tr>
  </tbody>
</table>
<p><strong>E9 (Frame Ablation)</strong>에서 4, 8, 16, 32 프레임 비교 → 최소 필요 프레임 수 도출</p>

<h3>7.3 비용 추정</h3>
<table>
  <thead>
    <tr><th>항목</th><th>세부</th><th>비용</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>GPT-4o API</strong></td>
      <td>260 videos x 3 strategies x 16 frames = ~$50-80</td>
      <td>$50-80</td>
    </tr>
    <tr>
      <td><strong>Gemini 2.5 Pro</strong></td>
      <td>260 videos x 4 strategies + few-shot = ~$30-50</td>
      <td>$30-50</td>
    </tr>
    <tr>
      <td><strong>Gemini Flash-Lite</strong></td>
      <td>대량 반복 실험 (ablation 등)</td>
      <td>$3-5</td>
    </tr>
    <tr>
      <td><strong>Open-source (HPC)</strong></td>
      <td>Qwen, InternVL, LLaVA 로컬 실행</td>
      <td>$0 (HPC)</td>
    </tr>
    <tr>
      <td><strong>AQA baseline 학습</strong></td>
      <td>CoRe/TSA on PD4T (HPC GPU)</td>
      <td>$0 (HPC)</td>
    </tr>
    <tr style="background:#f0f7ff; font-weight:600;">
      <td><strong>Total</strong></td>
      <td></td>
      <td><strong>$80-135</strong></td>
    </tr>
  </tbody>
</table>
</div>

<!-- Section 8: Expected Results -->
<div class="section">
<h2>8. Expected Results & Paper Story</h2>

<h3>8.1 예상 성능 (Pearson Correlation)</h3>
<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Gait</th>
      <th>Finger Tap</th>
      <th>Hand</th>
      <th>Leg</th>
      <th>Mean</th>
    </tr>
  </thead>
  <tbody>
    <tr style="color:#999;">
      <td>Random baseline</td>
      <td>~0.00</td>
      <td>~0.00</td>
      <td>~0.00</td>
      <td>~0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <td>VLM Zero-shot (Raw)</td>
      <td>~0.35</td>
      <td>~0.25</td>
      <td>~0.20</td>
      <td>~0.15</td>
      <td>~0.24</td>
    </tr>
    <tr>
      <td>VLM Zero-shot (CoT)</td>
      <td>~0.45</td>
      <td>~0.35</td>
      <td>~0.30</td>
      <td>~0.25</td>
      <td>~0.34</td>
    </tr>
    <tr>
      <td>VLM Few-shot (3-shot, CoT)</td>
      <td>~0.55</td>
      <td>~0.40</td>
      <td>~0.35</td>
      <td>~0.30</td>
      <td>~0.40</td>
    </tr>
    <tr style="background:#f0faf0;">
      <td><strong>VLM LoRA Fine-tuned</strong></td>
      <td><strong>~0.65</strong></td>
      <td><strong>~0.50</strong></td>
      <td><strong>~0.45</strong></td>
      <td><strong>~0.35</strong></td>
      <td><strong>~0.49</strong></td>
    </tr>
    <tr style="color:#999;">
      <td>AQA baseline (CoRe)</td>
      <td>~0.60</td>
      <td>~0.45</td>
      <td>~0.40</td>
      <td>~0.30</td>
      <td>~0.44</td>
    </tr>
    <tr style="background:#fff3e0;">
      <td>Traditional ML (CORAL)</td>
      <td>0.79</td>
      <td>0.55</td>
      <td>0.60</td>
      <td>0.24</td>
      <td>0.55</td>
    </tr>
    <tr style="background:#fff3e0;">
      <td>Traditional ML (Mamba Best)</td>
      <td>0.80</td>
      <td>0.61</td>
      <td>0.60</td>
      <td>0.31</td>
      <td>0.58</td>
    </tr>
  </tbody>
</table>

<h3>8.2 논문의 Core Story</h3>

<div class="box box-green">
<strong>Story 1 — "VLM은 의미 있게 작동한다"</strong><br>
Zero-shot VLM도 random보다 훨씬 높은 상관관계를 보임 (r~0.24 vs 0.00).<br>
→ VLM이 PD 운동 장애를 사전학습에서 학습했다는 증거.
</div>

<div class="box box-blue">
<strong>Story 2 — "입력 전략이 성능을 결정한다"</strong><br>
Clinical CoT (Strategy E)가 Raw Frames (Strategy A)보다 ~40% 개선.<br>
→ "무엇을 보여주는가"보다 "어떻게 판단하게 하는가"가 중요.
</div>

<div class="box box-purple">
<strong>Story 3 — "정확도 < 설명력, 그것이 VLM의 진짜 가치"</strong><br>
VLM은 Traditional ML보다 정확도가 낮지만, 자연어로 임상적 근거를 제공.<br>
→ Neurologist 평가에서 높은 임상 논리 점수 (E11).<br>
→ Black-box ML이 할 수 없는 "왜 이 점수인지" 설명 = 임상 수용성의 핵심.
</div>

<div class="box box-yellow">
<strong>Story 4 — "LoRA fine-tuning으로 격차 좁힐 수 있다"</strong><br>
Open-source VLM + LoRA = Traditional ML에 근접 (r~0.49 vs 0.55).<br>
→ 설명력 유지 + 정확도 개선 = 양쪽의 장점.<br>
→ 병원 로컬 배포 가능 (프라이버시 보호).
</div>

<h3>8.3 Key Figures (논문에 포함할 그림)</h3>
<table>
  <thead>
    <tr><th>Figure</th><th>내용</th><th>메시지</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Fig 1</strong></td>
      <td>Overview diagram: 6 strategies + 3 levels 시각화</td>
      <td>연구 구조 한눈에</td>
    </tr>
    <tr>
      <td><strong>Fig 2</strong></td>
      <td>Heatmap: Strategy x Model x Task 성능</td>
      <td>최적 조합 식별</td>
    </tr>
    <tr>
      <td><strong>Fig 3</strong></td>
      <td>Bar chart: Zero-shot vs Few-shot vs LoRA vs Traditional</td>
      <td>Adaptation level별 개선</td>
    </tr>
    <tr>
      <td><strong>Fig 4</strong></td>
      <td>Frame ablation curve (4→8→16→32 frames)</td>
      <td>최소 필요 프레임 수</td>
    </tr>
    <tr>
      <td><strong>Fig 5</strong></td>
      <td>VLM reasoning 예시 (Correct vs Incorrect)</td>
      <td>설명력 질적 분석</td>
    </tr>
    <tr>
      <td><strong>Fig 6</strong></td>
      <td>Radar chart: Accuracy vs Explainability vs Cost vs Privacy</td>
      <td>VLM vs ML 종합 비교</td>
    </tr>
    <tr>
      <td><strong>Fig 7</strong></td>
      <td>Commercial vs Open-source 성능 비교</td>
      <td>배포 가능성 분석</td>
    </tr>
  </tbody>
</table>
</div>

<!-- Section 9: Timeline -->
<div class="section">
<h2>9. Execution Timeline</h2>

<div class="timeline">
  <div class="timeline-item" style="border-left-color: #1976d2;">
    <div class="timeline-date">Phase 1: Foundation (Week 1-3)</div>
    <div class="timeline-desc">
      <strong>E1-E3: Zero-shot 실험 (Core)</strong><br>
      &bull; 프레임 추출 파이프라인 구축 (PD4T → 8/16 keyframes)<br>
      &bull; 프롬프트 설계 (Strategy A, B, E)<br>
      &bull; GPT-4o, Gemini Pro, Flash-Lite, Qwen, InternVL, LLaVA 실행<br>
      &bull; <strong>결과:</strong> 최적 모델 + 최적 strategy 1차 선정
    </div>
  </div>
  <div class="timeline-item" style="border-left-color: #43a047;">
    <div class="timeline-date">Phase 2: Expansion (Week 4-6)</div>
    <div class="timeline-desc">
      <strong>E4-E7, E9: 확장 실험</strong><br>
      &bull; E4-E6: 추가 strategies (C, D, F) zero-shot<br>
      &bull; E7: Few-shot 실험 (Gemini + Qwen, 1/3/5-shot)<br>
      &bull; E9: Frame ablation (4, 8, 16, 32 frames)<br>
      &bull; AQA baseline 학습 (CoRe on PD4T)
    </div>
  </div>
  <div class="timeline-item" style="border-left-color: #e65100;">
    <div class="timeline-date">Phase 3: Fine-tuning (Week 7-9)</div>
    <div class="timeline-desc">
      <strong>E8: LoRA Fine-tuning</strong><br>
      &bull; Qwen2.5-VL-7B LoRA 학습 (HPC)<br>
      &bull; Strategy A + E로 fine-tuned 모델 평가<br>
      &bull; TULIP으로 external validation<br>
      &bull; Fine-tuning 후 reasoning 품질 비교
    </div>
  </div>
  <div class="timeline-item" style="border-left-color: #7b1fa2;">
    <div class="timeline-date">Phase 4: Clinical Eval (Week 10-12)</div>
    <div class="timeline-desc">
      <strong>E10-E11: Baseline 비교 + 임상 평가</strong><br>
      &bull; 전체 baseline 비교 (Random, Majority, AQA, CORAL, Mamba)<br>
      &bull; Neurologist blind review (3명, 100건 reasoning 평가)<br>
      &bull; 논문 Figure/Table 생성
    </div>
  </div>
  <div class="timeline-item" style="border-left-color: #c62828;">
    <div class="timeline-date">Phase 5: Writing (Week 13-16)</div>
    <div class="timeline-desc">
      <strong>논문 작성 + 투고</strong><br>
      &bull; 논문 초고 작성<br>
      &bull; 내부 리뷰 + 수정<br>
      &bull; Target venue 투고
    </div>
  </div>
</div>

<p><strong>총 기간: ~4개월</strong> (실험 3개월 + 논문 1개월)</p>
</div>

<!-- Section 10: Contributions -->
<div class="section">
<h2>10. Expected Contributions</h2>

<div class="card-grid card-grid-2">
  <div class="card" style="border-left: 4px solid #c62828;">
    <h4>C1. First VLM Study for PD Motor Scoring</h4>
    <p>세계 최초로 VLM을 MDS-UPDRS Part III 운동 평가에 적용. Zero-shot부터 fine-tuning까지 체계적 평가.</p>
    <span class="tag tag-green">Novelty: Highest</span>
  </div>
  <div class="card" style="border-left: 4px solid #1976d2;">
    <h4>C2. Input Strategy Taxonomy</h4>
    <p>6가지 입력 전략의 체계적 비교. "VLM에게 무엇을 어떻게 보여줘야 하는가?"에 대한 실증적 답변. 다른 의료 영상 AI에도 적용 가능한 가이드라인.</p>
    <span class="tag tag-green">Novelty: High</span>
  </div>
  <div class="card" style="border-left: 4px solid #43a047;">
    <h4>C3. Explainability vs Accuracy Trade-off</h4>
    <p>VLM (낮은 정확도 + 높은 설명력) vs Traditional ML (높은 정확도 + black-box). 임상 AI 채택의 핵심 trade-off 정량화.</p>
    <span class="tag tag-blue">Impact: Very High</span>
  </div>
  <div class="card" style="border-left: 4px solid #7b1fa2;">
    <h4>C4. Clinical Deployment Roadmap</h4>
    <p>Commercial vs Open-source, 프라이버시 보호, LoRA adaptation. 병원 실제 배포를 위한 실용적 가이드.</p>
    <span class="tag tag-blue">Impact: High</span>
  </div>
</div>

<h3>10.1 vs Related Work Positioning</h3>
<table>
  <thead>
    <tr><th>기존 연구</th><th>한계</th><th>Our Contribution</th></tr>
  </thead>
  <tbody>
    <tr>
      <td>Yang et al. (GPT-4V for Surgery)</td>
      <td>수술 평가, PD 아님, 단일 VLM</td>
      <td>PD 특화 + 6 VLM 비교 + 입력 전략</td>
    </tr>
    <tr>
      <td>Duan et al. (VLM for AQA)</td>
      <td>스포츠 AQA, 의료 아님</td>
      <td>임상 프로토콜 기반 + Neurologist 평가</td>
    </tr>
    <tr>
      <td>CARE-PD (NeurIPS 2025)</td>
      <td>Skeleton/SMPL, Gait only, VLM 아님</td>
      <td>VLM 접근, 4 tasks, Explainability</td>
    </tr>
    <tr>
      <td>Morinan et al. (Movement Disorders)</td>
      <td>CNN black-box, 설명 불가</td>
      <td>VLM 자연어 설명 + 임상 수용성</td>
    </tr>
  </tbody>
</table>
</div>

<!-- Section 11: Risks -->
<div class="section">
<h2>11. Risks & Mitigation</h2>

<table>
  <thead>
    <tr><th>Risk</th><th>Level</th><th>Impact</th><th>Mitigation</th></tr>
  </thead>
  <tbody>
    <tr>
      <td>VLM Zero-shot 성능이 너무 낮음 (r&lt;0.15)</td>
      <td><span class="tag tag-orange">Medium</span></td>
      <td>논문 가치 감소</td>
      <td>Few-shot/LoRA로 개선 + "왜 실패하는지" 분석도 contribution</td>
    </tr>
    <tr>
      <td>GPT-4o API 비용 초과</td>
      <td><span class="tag tag-gray">Low</span></td>
      <td>실험 규모 축소</td>
      <td>Gemini Flash-Lite로 대량 실험, GPT-4o는 최소 실험만</td>
    </tr>
    <tr>
      <td>PD4T 데이터 접근 제한</td>
      <td><span class="tag tag-gray">Low</span></td>
      <td>실험 불가</td>
      <td>이미 보유 중, TULIP도 backup으로 확보</td>
    </tr>
    <tr>
      <td>VLM Hallucination (환자 상태 오인)</td>
      <td><span class="tag tag-orange">Medium</span></td>
      <td>임상 신뢰도</td>
      <td>Clinical CoT로 구조화 + hallucination rate 정량 보고</td>
    </tr>
    <tr>
      <td>Neurologist 평가 협조 어려움</td>
      <td><span class="tag tag-orange">Medium</span></td>
      <td>E11 축소</td>
      <td>최소 2명 확보, 100건→50건 축소 가능</td>
    </tr>
    <tr>
      <td>LoRA fine-tuning 후 reasoning 능력 저하</td>
      <td><span class="tag tag-orange">Medium</span></td>
      <td>C3 약화</td>
      <td>이 자체가 finding ("fine-tuning이 설명력을 파괴하는가?")</td>
    </tr>
  </tbody>
</table>
</div>

<!-- Section 12: Summary -->
<div class="section">
<h2>12. One-Page Summary</h2>

<div class="box box-blue" style="font-size:14px;">
  <p><strong>Paper Title:</strong> Can Vision-Language Models Score Parkinson's Disease? A Systematic Study of Input Strategies and Adaptation Methods for MDS-UPDRS Motor Assessment</p>
  <br>
  <p><strong>Gap:</strong> VLM for PD motor scoring = 0 papers worldwide. All existing work uses black-box ML.</p>
  <br>
  <p><strong>Method:</strong> 6 input strategies (Raw / Skeleton Overlay / Skeleton Only / + Kinematics / Clinical CoT / Multi-turn) x 3 adaptation levels (Zero-shot / Few-shot / LoRA) x 6 VLMs (3 commercial + 3 open-source)</p>
  <br>
  <p><strong>Data:</strong> PD4T (50 subjects, 4 tasks) + TULIP (15 subjects, external validation). Open datasets only.</p>
  <br>
  <p><strong>Baselines:</strong> Random, Majority, AQA (CoRe), Traditional ML (CORAL, Mamba)</p>
  <br>
  <p><strong>Key Findings (Expected):</strong></p>
  <ul>
    <li>VLM zero-shot significantly above random (r~0.24 vs 0.00)</li>
    <li>Clinical CoT >> Raw Frames (~40% improvement)</li>
    <li>LoRA fine-tuned open-source approaches traditional ML (r~0.49 vs 0.55)</li>
    <li>VLM reasoning rated clinically relevant by neurologists</li>
  </ul>
  <br>
  <p><strong>Impact:</strong> First evidence that VLMs can perform clinical motor assessment with explainable reasoning. Practical deployment roadmap for hospital use.</p>
  <br>
  <p><strong>Timeline:</strong> 4 months | <strong>Cost:</strong> ~$100 (API) | <strong>Target:</strong> Nature Medicine / Lancet Digital Health / MICCAI 2026</p>
</div>
</div>

</body>
</html>
